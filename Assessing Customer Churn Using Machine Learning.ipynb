{"cells":[{"source":"![Cartoon of telecom customers](IMG_8811.png)\n","metadata":{},"id":"cdd466f9-a72a-44df-9e00-6926a97a4923","cell_type":"markdown"},{"source":"The telecommunications (telecom) sector in India is rapidly changing, with more and more telecom businesses being created and many customers deciding to switch between providers. \"Churn\" refers to the process where customers or subscribers stop using a company's services or products. Understanding the factors that influence keeping a customer as a client in predicting churn is crucial for telecom companies to enhance their service quality and customer satisfaction. As the data scientist on this project, you aim to explore the intricate dynamics of customer behavior and demographics in the Indian telecom sector in predicting customer churn, utilizing two comprehensive datasets from four major telecom partners: Airtel, Reliance Jio, Vodafone, and BSNL:\n\n- `telecom_demographics.csv` contains information related to Indian customer demographics:\n\n| Variable             | Description                                      |\n|----------------------|--------------------------------------------------|\n| `customer_id `         | Unique identifier for each customer.             |\n| `telecom_partner `     | The telecom partner associated with the customer.|\n| `gender `              | The gender of the customer.                      |\n| `age `                 | The age of the customer.                         |\n| `state`                | The Indian state in which the customer is located.|\n| `city`                 | The city in which the customer is located.       |\n| `pincode`              | The pincode of the customer's location.          |\n| `registration_event` | When the customer registered with the telecom partner.|\n| `num_dependents`      | The number of dependents (e.g., children) the customer has.|\n| `estimated_salary`     | The customer's estimated salary.                 |\n\n- `telecom_usage` contains information about the usage patterns of Indian customers:\n\n| Variable   | Description                                                  |\n|------------|--------------------------------------------------------------|\n| `customer_id` | Unique identifier for each customer.                         |\n| `calls_made` | The number of calls made by the customer.                    |\n| `sms_sent`   | The number of SMS messages sent by the customer.             |\n| `data_used`  | The amount of data used by the customer.                     |\n| `churn`    | Binary variable indicating whether the customer has churned or not (1 = churned, 0 = not churned).|\n","metadata":{},"id":"dafa483a-e084-4ba8-9236-5c0468364e0d","cell_type":"markdown"},{"source":"# Import libraries and methods/functions\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression \nfrom sklearn.linear_model import RidgeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix\n\n# Start your code here!","metadata":{"executionCancelledAt":null,"executionTime":12,"lastExecutedAt":1717453713967,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Import libraries and methods/functions\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression \nfrom sklearn.linear_model import RidgeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix\n\n# Start your code here!","lastExecutedByKernel":"04d02f60-b61d-43b3-8727-591420ccd6ca"},"id":"95efd3c7-a48a-49c2-9df6-36f078de3b38","cell_type":"code","execution_count":64,"outputs":[]},{"source":"# Load the Data","metadata":{},"cell_type":"markdown","id":"45217b52-4b7b-4039-baa8-3e7b5d336a72"},{"source":"# Load data\ntelco_demog = pd.read_csv('telecom_demographics.csv')\ntelco_usage = pd.read_csv('telecom_usage.csv')\n","metadata":{"executionCancelledAt":null,"executionTime":50,"lastExecutedAt":1717453714017,"lastExecutedByKernel":"04d02f60-b61d-43b3-8727-591420ccd6ca","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Load data\ntelco_demog = pd.read_csv('telecom_demographics.csv')\ntelco_usage = pd.read_csv('telecom_usage.csv')\n"},"cell_type":"code","id":"42932361-9271-4260-a974-81fd66c25d36","outputs":[],"execution_count":65},{"source":"# Join the Data and Calculate Churn Rate\n\nIn this section, we will perform the following steps:\n\n1. **Join the Data**: Merge the demographic and usage data on the `customer_id` column to create a unified DataFrame named `churn_df`.\n2. **Identify Churn Rate**: Calculate the churn rate by determining the proportion of customers who have churned. This is done by using the `value_counts` method on the `churn` column and dividing by the total number of records in the DataFrame.\n\nThe code snippet below demonstrates these steps:","metadata":{},"cell_type":"markdown","id":"1ed5e174-a426-4b98-98bb-ba4a27ae711f"},{"source":"# Join data\nchurn_df = telco_demog.merge(telco_usage, on='customer_id')\n\n# Identify churn rate\nchurn_rate = churn_df['churn'].value_counts() / len(churn_df)\nprint(churn_rate)\n\n","metadata":{"executionCancelledAt":null,"executionTime":56,"lastExecutedAt":1717453714073,"lastExecutedByKernel":"04d02f60-b61d-43b3-8727-591420ccd6ca","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Join data\nchurn_df = telco_demog.merge(telco_usage, on='customer_id')\n\n# Identify churn rate\nchurn_rate = churn_df['churn'].value_counts() / len(churn_df)\nprint(churn_rate)\n\n","outputsMetadata":{"0":{"height":80,"type":"stream"}}},"cell_type":"code","id":"af7d47af-cbbc-4c9d-b8e7-adeda2922bbd","outputs":[{"output_type":"stream","name":"stdout","text":"0    0.799538\n1    0.200462\nName: churn, dtype: float64\n"}],"execution_count":66},{"source":"# Identify Categorical Variables and Perform One-Hot Encoding\n\nIn this section, we will identify the categorical variables in our unified DataFrame `churn_df` and perform one-hot encoding on these variables. One-hot encoding is a process that converts categorical variables into a format that can be provided to machine learning algorithms to improve predictions.\n\n1. **Identify Categorical Variables**: We will use the `info()` method to get a concise summary of the DataFrame, which includes the data types of each column. This will help us identify which columns are categorical.\n\n2. **One-Hot Encoding**: We will use the `pd.get_dummies()` function to perform one-hot encoding on the identified categorical variables. This function will create new binary columns for each category in the specified columns.\n\nThe code snippet below demonstrates these steps:","metadata":{},"cell_type":"markdown","id":"7f89211a-5056-4929-bade-8d4c3ac7cdf7"},{"source":"# Identify categorical variables\nprint(churn_df.info())\n\n# One Hot Encoding for categorical variables\nchurn_df = pd.get_dummies(churn_df, columns=['telecom_partner', 'gender', 'state', 'city', 'registration_event'])","metadata":{"executionCancelledAt":null,"executionTime":72,"lastExecutedAt":1717453714145,"lastExecutedByKernel":"04d02f60-b61d-43b3-8727-591420ccd6ca","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Identify categorical variables\nprint(churn_df.info())\n\n# One Hot Encoding for categorical variables\nchurn_df = pd.get_dummies(churn_df, columns=['telecom_partner', 'gender', 'state', 'city', 'registration_event'])","outputsMetadata":{"0":{"height":479,"type":"stream"}}},"cell_type":"code","id":"f2422659-fdef-4635-a488-8d1982c6aec7","outputs":[{"output_type":"stream","name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nInt64Index: 6500 entries, 0 to 6499\nData columns (total 14 columns):\n #   Column              Non-Null Count  Dtype \n---  ------              --------------  ----- \n 0   customer_id         6500 non-null   int64 \n 1   telecom_partner     6500 non-null   object\n 2   gender              6500 non-null   object\n 3   age                 6500 non-null   int64 \n 4   state               6500 non-null   object\n 5   city                6500 non-null   object\n 6   pincode             6500 non-null   int64 \n 7   registration_event  6500 non-null   object\n 8   num_dependents      6500 non-null   int64 \n 9   estimated_salary    6500 non-null   int64 \n 10  calls_made          6500 non-null   int64 \n 11  sms_sent            6500 non-null   int64 \n 12  data_used           6500 non-null   int64 \n 13  churn               6500 non-null   int64 \ndtypes: int64(9), object(5)\nmemory usage: 761.7+ KB\nNone\n"}],"execution_count":67},{"source":"# Feature Scaling\n\nFeature scaling is a crucial step in the data preprocessing pipeline, especially for algorithms that rely on distance metrics, such as k-nearest neighbors (KNN) and support vector machines (SVM). Scaling ensures that all features contribute equally to the model's performance by bringing them to a common scale without distorting differences in the ranges of values.\n\nIn this section, we will use the `StandardScaler` from the `sklearn.preprocessing` module to standardize our features. Standardization involves rescaling the features so that they have the properties of a standard normal distribution with a mean of zero and a standard deviation of one.\n\n1. **Initialize the Scaler**: We will create an instance of the `StandardScaler` class.\n2. **Select Features**: We will drop the `customer_id` and `churn` columns from our DataFrame `churn_df` as they are not features. The `customer_id` is an identifier, and `churn` is the target variable.\n3. **Fit and Transform**: We will fit the scaler to our features and transform them to the standardized scale.\n\nThe code snippet below demonstrates these steps:","metadata":{},"cell_type":"markdown","id":"bee992ca-e0f4-494f-9eb0-c6f79b355d44"},{"source":"# Feature Scaling\nscaler = StandardScaler()\n\n# 'customer_id' is not a feature\nfeatures = churn_df.drop(['customer_id', 'churn'], axis=1)\nfeatures_scaled = scaler.fit_transform(features)","metadata":{"executionCancelledAt":null,"executionTime":128,"lastExecutedAt":1717453714273,"lastExecutedByKernel":"04d02f60-b61d-43b3-8727-591420ccd6ca","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Feature Scaling\nscaler = StandardScaler()\n\n# 'customer_id' is not a feature\nfeatures = churn_df.drop(['customer_id', 'churn'], axis=1)\nfeatures_scaled = scaler.fit_transform(features)"},"cell_type":"code","id":"ff35a7ce-81eb-42c2-b542-eb3b69f880b9","outputs":[],"execution_count":68},{"source":"# Target Variable and Dataset Splitting\n\nAfter scaling our features, the next step is to define our target variable and split the dataset into training and testing sets. This is essential for evaluating the performance of our machine learning model.\n\n1. **Target Variable**: The target variable, which we aim to predict, is the `churn` column from our DataFrame `churn_df`.\n2. **Splitting the Dataset**: We will use the `train_test_split` function from the `sklearn.model_selection` module to split our dataset. This function will divide the scaled features and the target variable into training and testing sets. We will allocate 80% of the data for training and 20% for testing. Setting a `random_state` ensures reproducibility of the split.\n\nThe code snippet below demonstrates these steps:","metadata":{},"cell_type":"markdown","id":"aa39c427-720b-4148-b071-59389fa0c0d1"},{"source":"# Target variable\ntarget = churn_df['churn']\n\n# Splitting the dataset\nX_train, X_test, y_train, y_test = train_test_split(features_scaled, target, test_size=0.2, random_state=42)","metadata":{"executionCancelledAt":null,"executionTime":52,"lastExecutedAt":1717453714326,"lastExecutedByKernel":"04d02f60-b61d-43b3-8727-591420ccd6ca","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Target variable\ntarget = churn_df['churn']\n\n# Splitting the dataset\nX_train, X_test, y_train, y_test = train_test_split(features_scaled, target, test_size=0.2, random_state=42)"},"cell_type":"code","id":"b5e01dde-830d-4d8b-b26a-3744f9f67465","outputs":[],"execution_count":69},{"source":"# Logistic Regression Model\n\nLogistic Regression is a popular and widely used machine learning algorithm for binary classification problems. It models the probability that a given input point belongs to a certain class. In this section, we will:\n\n1. **Instantiate the Logistic Regression Model**: We will create an instance of the `LogisticRegression` class from the `sklearn.linear_model` module. Setting a `random_state` ensures the reproducibility of our results.\n2. **Fit the Model**: Using the training data (`X_train` and `y_train`), we will train the logistic regression model.\n3. **Make Predictions**: We will use the trained model to make predictions on the test data (`X_test`).\n4. **Evaluate the Model**: To assess the performance of our logistic regression model, we will use a confusion matrix and a classification report. These metrics will help us understand how well our model is performing in terms of precision, recall, F1-score, and overall accuracy.\n\nThe code snippet below demonstrates these steps:","metadata":{},"cell_type":"markdown","id":"afe1de21-850b-41cc-994a-35a249a0eadb"},{"source":"# Instantiate the Logistic Regression\nlogreg = LogisticRegression(random_state=42)\nlogreg.fit(X_train, y_train)\n\n# Logistic Regression predictions\nlogreg_pred = logreg.predict(X_test)\n\n# Logistic Regression evaluation\nprint(confusion_matrix(y_test, logreg_pred))\nprint(classification_report(y_test, logreg_pred))","metadata":{"executionCancelledAt":null,"executionTime":527,"lastExecutedAt":1717453714855,"lastExecutedByKernel":"04d02f60-b61d-43b3-8727-591420ccd6ca","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Instantiate the Logistic Regression\nlogreg = LogisticRegression(random_state=42)\nlogreg.fit(X_train, y_train)\n\n# Logistic Regression predictions\nlogreg_pred = logreg.predict(X_test)\n\n# Logistic Regression evaluation\nprint(confusion_matrix(y_test, logreg_pred))\nprint(classification_report(y_test, logreg_pred))","outputsMetadata":{"0":{"height":248,"type":"stream"}}},"cell_type":"code","id":"9fe5d1f8-96b3-4c01-9a96-eadce0351690","outputs":[{"output_type":"stream","name":"stdout","text":"[[920 107]\n [245  28]]\n              precision    recall  f1-score   support\n\n           0       0.79      0.90      0.84      1027\n           1       0.21      0.10      0.14       273\n\n    accuracy                           0.73      1300\n   macro avg       0.50      0.50      0.49      1300\nweighted avg       0.67      0.73      0.69      1300\n\n"}],"execution_count":70},{"source":"# Random Forest Model\n\nRandom Forest is an ensemble learning method that operates by constructing multiple decision trees during training and outputting the class that is the mode of the classes of the individual trees. It is known for its robustness and ability to handle large datasets with higher dimensionality. In this section, we will:\n\n1. **Instantiate the Random Forest Model**: We will create an instance of the `RandomForestClassifier` class from the `sklearn.ensemble` module. Setting a `random_state` ensures the reproducibility of our results.\n2. **Fit the Model**: Using the training data (`X_train` and `y_train`), we will train the random forest model.\n3. **Make Predictions**: We will use the trained model to make predictions on the test data (`X_test`).\n4. **Evaluate the Model**: To assess the performance of our random forest model, we will use a confusion matrix and a classification report. These metrics will help us understand how well our model is performing in terms of precision, recall, F1-score, and overall accuracy.\n\nThe code snippet below demonstrates these steps:","metadata":{},"cell_type":"markdown","id":"296dc3ca-3eac-416d-acff-2a32bd456f47"},{"source":"# Instantiate the Random Forest model\nrf = RandomForestClassifier(random_state=42)\nrf.fit(X_train, y_train)\n\n# Random Forest predictions\nrf_pred = rf.predict(X_test)\n\n# Random Forest evaluation\nprint(confusion_matrix(y_test, rf_pred))\nprint(classification_report(y_test, rf_pred))","metadata":{"executionCancelledAt":null,"executionTime":3598,"lastExecutedAt":1717453718453,"lastExecutedByKernel":"04d02f60-b61d-43b3-8727-591420ccd6ca","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Instantiate the Random Forest model\nrf = RandomForestClassifier(random_state=42)\nrf.fit(X_train, y_train)\n\n# Random Forest predictions\nrf_pred = rf.predict(X_test)\n\n# Random Forest evaluation\nprint(confusion_matrix(y_test, rf_pred))\nprint(classification_report(y_test, rf_pred))","outputsMetadata":{"0":{"height":248,"type":"stream"}}},"cell_type":"code","id":"22a32f0d-d00e-4cd9-a6c7-5aa9a6f0594f","outputs":[{"output_type":"stream","name":"stdout","text":"[[1026    1]\n [ 273    0]]\n              precision    recall  f1-score   support\n\n           0       0.79      1.00      0.88      1027\n           1       0.00      0.00      0.00       273\n\n    accuracy                           0.79      1300\n   macro avg       0.39      0.50      0.44      1300\nweighted avg       0.62      0.79      0.70      1300\n\n"}],"execution_count":71},{"source":"### Comparison\n\n#### Accuracy:\n- **Random Forest Model:** 0.79\n- **Logistic Regression:** 0.73\n\n#### Class 0 (Majority Class):\n- **Random Forest Model F1-score:** 0.88\n- **Logistic Regression F1-score:** 0.84\n\n#### Class 1 (Minority Class):\n- **Random Forest Model F1-score:** 0.00\n- **Logistic Regression F1-score:** 0.14\n\n#### Macro Average F1-score:\n- **Random Forest Model:** 0.44\n- **Logistic Regression:** 0.49\n\n#### Weighted Average F1-score:\n- **Random Forest Model:** 0.70\n- **Logistic Regression:** 0.69\n\n### Interpretation\n\n- **Accuracy:** The random forest model has a higher accuracy (0.79) compared to the logistic regression model (0.73). However, accuracy is not a reliable metric when dealing with imbalanced datasets.\n\n- **Class 0 Performance:** Both models perform well for the majority class (class 0), but the random forest model performs slightly better with a higher F1-score (0.88 vs. 0.84).\n\n- **Class 1 Performance:** The logistic regression model performs better for the minority class (class 1) with a non-zero F1-score (0.14 vs. 0.00). This indicates that logistic regression has a better ability to identify instances of the minority class, although it is still poor.\n\n- **Macro and Weighted Average:** The logistic regression model has a slightly better macro average F1-score (0.49 vs. 0.44) but a slightly lower weighted average F1-score (0.69 vs. 0.70).","metadata":{},"cell_type":"markdown","id":"4348f058-0e02-4f7e-a4f7-78d4ed719851"},{"source":"# Conclusion\nWith respect to accuracy, the random forest model is the winner, as it has a higher accuracy (0.79) compared to the logistic regression model (0.73). However, considering the imbalanced nature of the dataset, other metrics like F1-score for the minority class should also be taken into account for a comprehensive evaluation.","metadata":{},"cell_type":"markdown","id":"1d9e5947-4f58-495b-b243-e231f1e22227"},{"source":"# Which accuracy score is higher? Logistic or RandomForest\nhigher_accuracy = \"RandomForest\"","cell_type":"code","id":"cf0ac695-755c-4611-ae69-06fd4ee588ca","outputs":[],"metadata":{"executionCancelledAt":null,"executionTime":52,"lastExecutedAt":1717453718505,"lastExecutedByKernel":"04d02f60-b61d-43b3-8727-591420ccd6ca","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Which accuracy score is higher? Logistic or RandomForest\nhigher_accuracy = \"RandomForest\""},"execution_count":72}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":5}